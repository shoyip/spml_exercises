---
title: 'Exercise 2: Ranking binary numbers with a perceptron'
jupyter: python3
---


A **perceptron** takes two binary numbers of 10 digits each and returns either +1 or -1.

$\vec S$ has 20 elements ($\pm 1$). The first 10 elements are the **top** dataset $n_t$ and the others are the **bottom** dataset $n_b$.

$$
\sigma (\{S_i^{\mu}\} = \{\text{sgn}(\sum_i S_i J_i)\})
$$

The goal of the perceptron is to **tell whether $n_t > n_b$ or viceversa**.


```{python}
import numpy as np
import matplotlib.pyplot as plt
```

## Step 1: Set up the perfect perceptron

Let us define the perfect perceptron as the function that given the following weights array

```{python}
def get_pp_weights():
    J_star = np.zeros(20)
    for i in range(1,21):
        J_star[i-1] = 2**(10 - i) if i<=10 else -J_star[i-11]
    return J_star
```

```{python}
get_pp_weights()
```

returns the dot product between the weights and the input array

$$
\sigma = \vec w \cdot \vec s
$$

which amounts to comparing the two numbers in binary representation.

```{python}
def perfect_perceptron(weights, input):
    return int(np.sign(np.dot(weights, input)))
```

## Step 2: Define and evaluate a test error $\epsilon$ of the PP

We compare said number with the true value, which we choose to be the function such that it returns +1 if the integer comparison between $n_t$ and $n_b$ is such that $n_t \geq n_b$, and -1 otherwise.

In a sense, we are checking whether the comparison in decimal representation is the same as the comparison in binary representation.

$$
\sigma_{\text{true}} = \text{sgn}(n_t - n_b)
$$

except when $n_t = n_b$ this number yields +1.

**Notice** that I have inserted the definition for the **equality** case, otherwise it would get an error point due to the fact that the binary comparison rule does not account for the equality case.

```{python}
def to_int(binarr):
    return int(''.join(list(map(str, binarr))), 2)

def comparison_sign(num1, num2):
    if num1==num2:
        return 0
    elif (num1<num2):
        return -1
    else:
        return +1

def comparison_binarr(binarr):
    # compare binaries as ints
    num1 = to_int(binarr[:10])
    num2 = to_int(binarr[10:])
    return int(comparison_sign(num1, num2))
```

```{python}
inputs = np.random.randint(0, 2, size=(100, 20))
print('inputs =\n', inputs, '\n\ninputs is a matrix of shape', inputs.shape)
```

```{python}
weights = get_pp_weights()
print('weights =\n', weights, '\n\nweights is a matrix of shape', weights.shape)
```

We may define a trivial error measure such as

$$
\epsilon = |\sigma - \sigma_{\text{true}}|
$$

Let us compare, for example, 100 instances of this comparison.

```{python}
# print('sigma_true\tsigma\teps')
eps_tot_list = []
for j in range(100):
    # weights are already defined
    inputs = np.random.randint(0, 2, size=(100, 20))
    epsilon_sum = 0
    for i in range(len(inputs)):
        sigma_true = comparison_binarr(inputs[i])
        sigma = perfect_perceptron(weights, inputs[i])
        epsilon = abs(sigma_true - sigma)
        epsilon_sum += epsilon
        # print(sigma_true, '\t\t', sigma, '\t', epsilon)
    
    eps_tot = epsilon_sum
    eps_tot_list.append(eps_tot)
    # print('The error is', eps_tot)
```

```{python}
plt.figure()
plt.title('Histogram of error values ')
plt.hist(eps_tot_list)
plt.show()
plt.close()
```

We see that we have exactly error 0 on the 100 runs, each with 100 samples. The **perfect perceptron** is, in fact, perfect (with the caveat of the definition of the *border* situation). The error is a delta function in 0, which is reasonable.

## Step 3: Assign random synaptic weights

Until now we had a *perfect perceptron*, meaning that we used a perceptron where we knew the model (the knowledge of the specific weights that would reproduce the comparison behaviour) that would trustfully yield the correct data.

Now let us show instead a real learning process, where we do not have any prior knowledge. Weights are, in fact, extracted from $\mathcal{N}(0, 1)$.

Our previous perceptron had 20 weights. Likewise, we find 20 Gaussian weights in this situation as well.

```{python}
inputs = np.random.randint(0, 2, size=(100, 20))
print('inputs =\n', inputs, '\n\ninputs is a matrix of shape', inputs.shape)
```

```{python}
weights = np.random.normal(0, 1, size=(20))
print("weights =\n", weights)
print("\n\nweights is a matrix of shape ", weights.shape)
```

For now we have not done any learning, so I do not expect a good result. Instead, I expect that half of the times we get the right answer, and the other half of the times we get it wrong.

Let us repeat the evaluation with the errors as we have done in Step 2.

```{python}
# print('sigma_true\tsigma\teps')
eps_tot_list = []
for j in range(1000):
    # weights are already defined
    inputs = np.random.randint(0, 2, size=(100, 20))
    epsilon_sum = 0
    for i in range(len(inputs)):
        sigma_true = comparison_binarr(inputs[i])
        sigma = perfect_perceptron(weights, inputs[i])
        epsilon = abs(sigma_true - sigma)
        epsilon_sum += epsilon
        # print(sigma_true, '\t\t', sigma, '\t', epsilon)
    
    eps_tot = epsilon_sum * 1. / (2*len(inputs))
    eps_tot_list.append(eps_tot)
    # print('The error is', eps_tot)
```

```{python}
plt.figure()
plt.title('Histogram of error values ')
plt.hist(eps_tot_list)
plt.show()
plt.close()
```

We notice that over many runs (100 runs), each made of 100 samples, the (relative) error is centered around 0.5, meaning that the hypothesis that about half of the times we get it right and half wrong was correct.

## Step 5: Produce $\vec \xi^{\mu}$ for learning

So we until know have used the so-called *perfect perceptron* and the *random perceptron*. Let us know treat the case for learning.
In order to do this we should produce a **training set**.

The problem is the following: we have $p$ input vectors $\vec \xi^{\mu}$, where $\mu=1, ..., p$. These $p$ vectors are a random subsample of the configuration space. Each $\mu$-th data point has $\sigma_T^{\mu}$ value $\pm 1$, which is the *label*.

We want two datasets, dataset 1 and 2, respectively containing $P_1=500$ and $P_2=2000$  points, containing two numbers (the x, encoded as a vector 20 binary digits long) and a value $\pm 1$ (the y). We can use the decimal conversion as well to create the dataset, since we know that the perfect perceptron coincides with the integer comparison from Step 2.

We should however redefine the comparison, remember the caveat about the $n_t=n_b$ case. We choose to include it with the $>$ case.

```{python}
def comparison_sign_arr(binarr):
    num1 = to_int(binarr[:10])
    num2 = to_int(binarr[10:])
    
    if (num1<num2):
        return -1
    else:
        return +1

P1 = 500
P2 = 2000

x_train_1 = np.random.randint(0, 2, size=(P1, 20))
x_train_2 = np.random.randint(0, 2, size=(P2, 20))
y_train_1 = np.array(list(map(comparison_sign_arr, x_train_1)))
y_train_2 = np.array(list(map(comparison_sign_arr, x_train_2)))
```

## Step 6.1: Training with the first set

Let us again initialise some random weights.

```{python}
weights1 = np.random.normal(0, 1, size=(20))
print("weights =\n", weights1)
print("\n\nweights is a matrix of shape ", weights1.shape)
```

```{python}
# we can include the zero condition like this
# np.sign(0) + (0==0)
def my_sign(num):
    return np.sign(num) + (num==0)

prefact = 1./np.sqrt(20)
```

And let us now update the weights for learning. We are using the following rule

$$
\vec J (t+1) = \vec J(t) + \frac{1}{\sqrt{N}} \sigma_T^{\mu} \odot \xi^{\mu} \quad \text{ if } \quad \sigma^{\mu} = \sigma(\vec \xi^{\mu}, \vec J) \neq \sigma_T^{\mu}
$$

which is *kind of a* gradient descent. In fact if the true value and the predicted value are not the same, we try to go to a *lower energy* situation by adding a contribution to the weights given by the *element-wise product* ($\cdot$) between the current training data input $x^{\mu}$ instance and the true label $\sigma_T^{\mu}$. This *pushes* the values of the weights in the $p$-dimensional direction where the true values are lying.

The error measure we use for each run is the following:

$$
\epsilon = \frac{|\sigma_{\text{true}} - \sigma_{\text{pred}}|}{2}
$$

We perform several runs $N_{\text{runs}}$, and in each run we scan through the entire $P$-sized dataset. The general error is

$$
\epsilon_{\text{tot}} = \frac{\sum_{j=1}^{N_{\text{runs}}} \epsilon_j}{\sqrt{N_{\text{runs}}}}
$$

We use a naif stopping condition (we stop whenever the error reaches a zero value once), so we are not reaching *full convergence*, but it should be a sufficient convergence criterion for this simple example. Let's see.

For each time step the convergence process of $\epsilon$ is the following:

```{python}
epsilon1_t = []
epsilon = 1 # arbitrary error to start the cycle
while epsilon > 0:
    epsilon_sum = 0
    for mu in range(P1):
        y_pred = my_sign(x_train_1[mu] @ weights1)
        if y_pred != y_train_1[mu]:
            weights1 += prefact * y_train_1[mu] * x_train_1[mu]
            #print(prefact * y_train_1[mu] * x_train_1[mu])
        epsilon_mu = abs(y_pred - y_train_1[mu])
        epsilon_sum += epsilon_mu / 2.
        #print(epsilon)
    epsilon = epsilon_sum / np.sqrt(P1)
    epsilon1_t.append(epsilon)
    #print(epsilon, end=', ')
```

```{python}
plt.figure()
plt.scatter(range(len(epsilon1_t)), epsilon1_t, s=1)
plt.xlabel('run timestep')
plt.ylabel('$\epsilon$')
plt.grid()
plt.show()
```

Let's see our weights after they have been updated during the training phase.

```{python}
print("weights =\n", weights1)
print("\n\nweights is a matrix of shape ", weights1.shape)
```

```{python}
pp_weights = get_pp_weights()
```

```{python}
np.multiply(weights1, pp_weights) * np.log2(np.abs(weights1))
```

```{python}
plt.figure()
plt.bar(range(20), my_sign(np.multiply(weights1, pp_weights)) * np.log2(np.abs(weights1)))
plt.xlabel('i index')
plt.ylabel('sgn$(J_i^* J_i)$ $\log_2(|J_i|)$')
plt.grid()
plt.xticks(list(range(0, 20)))
plt.show()
```

## Step 6.2: Training with the second set

```{python}
weights2 = np.random.normal(0, 1, size=(20))
print("weights =\n", weights2)
print("\n\nweights is a matrix of shape ", weights2.shape)
```

```{python}
epsilon2_t = []
epsilon = 1 # arbitrary error to start the cycle
while epsilon > 0:
    epsilon_sum = 0
    for mu in range(P2):
        #print(epsilon)
        y_pred = my_sign(x_train_2[mu] @ weights2)
        if y_pred != y_train_2[mu]:
            weights2 += prefact * y_train_2[mu] * x_train_2[mu]
        epsilon_mu = abs(y_pred - y_train_2[mu])
        epsilon_sum += epsilon_mu / 2.
    epsilon = epsilon_sum / np.sqrt(P2)
    #print(epsilon, end=', ')
    epsilon2_t.append(epsilon)
```

```{python}
plt.figure()
plt.scatter(range(len(epsilon2_t)), epsilon2_t, s=1)
plt.xlabel('run timestep')
plt.ylabel('$\epsilon$')
plt.grid()
plt.show()
```

Let's see our weights after they have been updated during the training phase.

```{python}
print("weights =\n", weights2)
print("\n\nweights is a matrix of shape ", weights.shape)
```

```{python}
plt.figure()
plt.bar(range(20), my_sign(np.multiply(weights2, pp_weights)) * np.log2(np.abs(weights2)))
plt.xlabel('i index')
plt.ylabel('sgn$(J_i^* J_i)$ $\log_2(|J_i|)$')
plt.grid()
plt.xticks(list(range(0, 20)))
plt.show()
```

## Step 7: Comment on the metrics of the training step

The metrics used in Step 6 the most relevant weights are more or less in descending order 0, 1, 2, ... and in a parallel manner entries with $i$ given by 10, 11, 12, ... This makes sense, since it means that binary positions with higher weights are more relevant in deciding whether a number is bigger or not with respect to another. As we get into positions referring to smaller numbers, we loose information (i.e. we do not have anymore a monotonically decreasing behaviour).

This statistics becomes more consistent as the size $p$ of the training set is increased, as we can see by comparing the plots of the two cases $p=500$ and $p=2000$. From a computational point of view, as expected, the training of the bigger set takes more time to be completed.

## Step 8: Evaluate the test error $\epsilon$ on the trained perceptrons

Now we evaluate the trained perceptrons over unseen test datasets. How they will perform?

We use the usual error measure

$$
\epsilon = \frac{|\sigma_{\text{true}} - \sigma_{\text{pred}}|}{2}
$$

this time normalized also over the number of samples in the current run, in order to get statistics.

We can see this time that the distribution of errors over the 1000 runs is now much improved with respect to the random case, where recall that we saw a peak around 0.5 (half of the time we got it right, half wrong). Here instead we have a peak at values $<0.1$, meaning that we have much improved performances.

```{python}
eps_tot_list = []
for j in range(1000):
    # weights are already defined
    inputs = np.random.randint(0, 2, size=(100, 20))
    epsilon_sum = 0
    for i in range(len(inputs)):
        sigma_true = comparison_binarr(inputs[i])
        sigma_pred = my_sign(weights1 @ inputs[i])
        epsilon = abs(sigma_true - sigma_pred)
        epsilon_sum += epsilon
        # print(sigma_true, '\t\t', sigma, '\t', epsilon)
    
    eps_tot = epsilon_sum * 1. / (2*len(inputs))
    eps_tot_list.append(eps_tot)
```

```{python}
plt.figure()
plt.title('Histogram of test error for $p=500$')
plt.hist(eps_tot_list)
plt.xlim([0, 1])
plt.show()
plt.close()
```

The distribution of the test errors for the $p=2000$ case, with 1000 runs is given by the following histogram.

```{python}
eps_tot_list = []
for j in range(1000):
    # weights are already defined
    inputs = np.random.randint(0, 2, size=(100, 20))
    epsilon_sum = 0
    for i in range(len(inputs)):
        sigma_true = comparison_binarr(inputs[i])
        sigma_pred = my_sign(weights2 @ inputs[i])
        epsilon = abs(sigma_true - sigma_pred)
        epsilon_sum += epsilon
        # print(sigma_true, '\t\t', sigma, '\t', epsilon)
    
    eps_tot = epsilon_sum * 1. / (2*len(inputs))
    eps_tot_list.append(eps_tot)
```

By increasing the size of the training set the generalization gets better and better, reaching really small values as it can be seen in the following plot.

```{python}
plt.figure()
plt.title('Histogram of test error for $p=2000$')
plt.hist(eps_tot_list)
plt.xlim([0, 1])
plt.show()
plt.close()
```

