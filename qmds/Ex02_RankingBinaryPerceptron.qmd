---
title: 'Exercise 2: Ranking binary numbers with a perceptron'
jupyter: python3
---


A **perceptron** takes two binary numbers of 10 digits each and returns either +1 or -1.

$\vec S$ has 20 elements ($\pm 1$). The first 10 elements are the **top** dataset $n_t$ and the others are the **bottom** dataset $n_b$.

$$
\sigma (\{S_i^{\mu}\} = \{\text{sgn}(\sum_i S_i J_i)\})
$$

The goal of the perceptron is to **tell whether $n_t > n_b$ or viceversa**.


```{python}
import numpy as np
import matplotlib.pyplot as plt
```

## Step 1: Set up the perfect perceptron

Let us define the perfect perceptron as the function that given the following weights array

```{python}
def pp_weights():
    J_star = np.zeros(20)
    for i in range(1,21):
        J_star[i-1] = 2**(10 - i) if i<=10 else -J_star[i-11]
    return J_star
```

```{python}
pp_weights()
```

returns the dot product between the weights and the input array

$$
\sigma = \vec w \cdot \vec s
$$

which amounts to comparing the two numbers in binary representation.

```{python}
def perfect_perceptron(weights, input):
    return int(np.sign(np.dot(weights, input)))
```

## Step 2: Define and evaluate a test error $\epsilon$ of the PP

We compare said number with the true value, which we choose to be the function such that it returns +1 if the integer comparison between $n_t$ and $n_b$ is such that $n_t \geq n_b$, and -1 otherwise.

In a sense, we are checking whether the comparison in decimal representation is the same as the comparison in binary representation.

$$
\sigma_{\text{true}} = \text{sgn}(n_t - n_b)
$$

except when $n_t = n_b$ this number yields +1.

**Notice** that I have inserted the definition for the **equality** case, otherwise it would get an error point due to the fact that the binary comparison rule does not account for the equality case.

```{python}
def to_int(binarr):
    return int(''.join(list(map(str, binarr))), 2)

def comparison_sign(num1, num2):
    if num1==num2:
        return 0
    elif (num1<num2):
        return -1
    else:
        return +1

def comparison_binarr(binarr):
    # compare binaries as ints
    num1 = to_int(binarr[:10])
    num2 = to_int(binarr[10:])
    return int(comparison_sign(num1, num2))
```

```{python}
inputs = np.random.randint(0, 2, size=(100, 20))
print('inputs =\n', inputs, '\n\ninputs is a matrix of shape', inputs.shape)
```

```{python}
weights = pp_weights()
print('weights =\n', weights, '\n\nweights is a matrix of shape', weights.shape)
```

We may define a trivial error measure such as

$$
\epsilon = |\sigma - \sigma_{\text{true}}|
$$

Let us compare, for example, 100 instances of this comparison.

```{python}
# print('sigma_true\tsigma\teps')
eps_tot_list = []
for j in range(100):
    # weights are already defined
    inputs = np.random.randint(0, 2, size=(100, 20))
    epsilon_sum = 0
    for i in range(len(inputs)):
        sigma_true = comparison_binarr(inputs[i])
        sigma = perfect_perceptron(weights, inputs[i])
        epsilon = abs(sigma_true - sigma)
        epsilon_sum += epsilon
        # print(sigma_true, '\t\t', sigma, '\t', epsilon)
    
    eps_tot = epsilon_sum
    eps_tot_list.append(eps_tot)
    # print('The error is', eps_tot)
```

```{python}
plt.figure()
plt.title('Histogram of error values ')
plt.hist(eps_tot_list)
plt.show()
plt.close()
```

We see that we have exactly error 0 on the 100 runs, each with 100 samples. The **perfect perceptron** is, in fact, perfect (with the caveat of the definition of the *border* situation).

## Step 3: Assign random synaptic weights

Until now we had a *perfect perceptron*, meaning that we used a perceptron where we knew the model (the knowledge of the specific weights that would reproduce the comparison behaviour) that would trustfully yield the correct data.

Now let us show instead a real learning process, where we do not have any prior knowledge. Weights are, in fact, extracted from $\mathcal{N}(0, 1)$.

Our previous perceptron had 20 weights. Likewise, we find 20 Gaussian weights in this situation as well.

```{python}
inputs = np.random.randint(0, 2, size=(100, 20))
print('inputs =\n', inputs, '\n\ninputs is a matrix of shape', inputs.shape)
```

```{python}
weights = np.random.normal(0, 1, size=(20))
print("weights =\n", weights)
print("\n\nweights is a matrix of shape ", weights.shape)
```

For now we have not done any learning, so I do not expect a good result. Instead, I expect that half of the times we get the right answer, and the other half of the times we get it wrong.

Let us repeat the evaluation with the errors as we have done in Step 2.

```{python}
# print('sigma_true\tsigma\teps')
eps_tot_list = []
for j in range(100):
    # weights are already defined
    inputs = np.random.randint(0, 2, size=(100, 20))
    epsilon_sum = 0
    for i in range(len(inputs)):
        sigma_true = comparison_binarr(inputs[i])
        sigma = perfect_perceptron(weights, inputs[i])
        epsilon = abs(sigma_true - sigma)
        epsilon_sum += epsilon
        # print(sigma_true, '\t\t', sigma, '\t', epsilon)
    
    eps_tot = epsilon_sum * 1. / (2*len(inputs))
    eps_tot_list.append(eps_tot)
    # print('The error is', eps_tot)
```

```{python}
plt.figure()
plt.title('Histogram of error values ')
plt.hist(eps_tot_list)
plt.show()
plt.close()
```

We notice that over many runs (100 runs), each made of 100 samples, the (relative) error is centered around 0.5, meaning that the hypothesis that about half of the times we get it right and half wrong was correct.

## Step 5: Produce $\vec \xi^{\mu}$ for learning

Now we want to iterate through a wide dataset and see whether we learn anything. In order to do this we should produce a **training set**.

We want two datasets, dataset 1 and 2, respectively containing $P_1=500$ and $P_2=2000$  points, containing two numbers (the x, encoded as a vector 20 binary digits long) and a value $\pm 1$ (the y). We can use the decimal conversion as well to create the dataset, since we know that the perfect perceptron coincides with the integer comparison from Step 2.

We should however redefine the comparison, remember the caveat about the $n_t=n_b$ case. We choose to include it with the $>$ case.

```{python}
def comparison_sign_arr(binarr):
    num1 = to_int(binarr[:10])
    num2 = to_int(binarr[10:])
    
    if (num1<num2):
        return -1
    else:
        return +1

P1 = 500
P2 = 2000

x_train_1 = np.random.randint(0, 2, size=(P1, 20))
x_train_2 = np.random.randint(0, 2, size=(P2, 20))
y_train_1 = np.array(list(map(comparison_sign_arr, x_train_1)))
y_train_2 = np.array(list(map(comparison_sign_arr, x_train_2)))
```

## Step 6.1: Training with the first set

Let us again initialise some random weights.

```{python}
weights = np.random.normal(0, 1, size=(20))
print("weights =\n", weights)
print("\n\nweights is a matrix of shape ", weights.shape)
```

```{python}
# we can include the zero condition like this
# np.sign(0) + (0==0)
def my_sign(num):
    return np.sign(num) + (num==0)

prefact = 1./np.sqrt(20)
```

```{python}
for mu in range(P1):
    y_pred = my_sign(x_train_1[mu] @ weights)
    if y_pred != y_train_1[mu]:
        weights += prefact * y_train_1[mu] * x_train_1[mu]
    epsilon = abs(y_pred - y_train_1[mu])
    print(epsilon)
```

Let's see our weights after they have been updated during the training phase.

```{python}
print("weights =\n", weights)
print("\n\nweights is a matrix of shape ", weights.shape)
```

```{python}
pp_weights = pp_weights()
```

```{python}
np.multiply(weights, pp_weights) * np.log2(np.abs(weights))
```

```{python}
#| scrolled: true
plt.figure()
plt.scatter(range(20), np.multiply(weights, pp_weights) * np.log2(np.abs(weights)))
plt.grid()
plt.xticks(list(range(0, 20)))
plt.show()
```

**VECTOR with P2**

```{python}
weights = np.random.normal(0, 1, size=(20))
print("weights =\n", weights)
print("\n\nweights is a matrix of shape ", weights.shape)
```

```{python}
for mu in range(P2):
    y_pred = my_sign(x_train_2[mu] @ weights)
    if y_pred != y_train_2[mu]:
        weights += prefact * y_train_2[mu] * x_train_2[mu]
    epsilon = abs(y_pred - y_train_2[mu])
    print(epsilon)
```

Let's see our weights after they have been updated during the training phase.

```{python}
#| scrolled: true
print("weights =\n", weights)
print("\n\nweights is a matrix of shape ", weights.shape)
```

```{python}
#| scrolled: true
plt.figure()
plt.scatter(range(20), np.multiply(weights, pp_weights) * np.log2(np.abs(weights)))
plt.grid()
plt.xticks(list(range(0, 20)))
plt.show()
```

