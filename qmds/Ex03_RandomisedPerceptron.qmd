---
title: 'Exercise 03: The randomised perceptron learning rule'
jupyter: python3
---


We consider a more general case of a perceptron in this exercise.

- First of all, we consider a **teacher**-**student** scenario, which is more general and allows us to better explain the nuts and bolts of the learning process.
- Also, we set an $N$ generic number of (obviously even) elements of the input vector.
- We make learning noisy, by adding a random Gaussian term to the learning rule.

## Step 1: Use the perfect perceptron as T

The basic setting is the following: we have a teacher perceptron, which in our case will be the already seen *perfect perceptron*. The perceptron is given by

$$
\sigma(\vec \xi^{\mu}, \vec J) = \text{sign}(\vec \xi^{\mu} \cdot \vec J)
$$

and the weights of the perfect perceptron are $J_i^* = 2^{N/2-i}$ in the first half of the weights vector and the same numbers but of the opposite sign in the rest.

## Step 2: Initialise a student perceptron with random weights

In the **training** part, the initial weights will be initialised at random for the student perceptron. It will be attempting learning from the teacher perceptron by applying a learning rule, which is in our case

$$
\vec J (t+1) = \vec J + \frac{1}{\sqrt{N}} \sigma^{\mu}_T \vec \xi^{\mu} (1+ r)
$$

where $r\sim\mathcal{N}(0, \sqrt{50})$.


```{python}
import numpy as np
import matplotlib.pyplot as plt
import multiprocessing
from functools import partial
from joblib import Parallel, delayed
import time
```

```{python}
def my_sign(x):
    # x is a numpy array
    return 2 * (x >= 0) - 1

def get_pp_weights(N):
    if N%2!=0:
        raise ValueError('N value should be even and not odd.')
    half_N = int(N/2)
    J_star = np.zeros(N)
    for i in range(1, N+1):
        J_star[i-1] = 2**(half_N - i) if i<=half_N else -J_star[i-(half_N+1)]
    return J_star

def perceptron(x, weights):
    return my_sign(x @ weights)
```

```{python}
N = 20

pp_weights = get_pp_weights(N)

print('Currently N is', N, 'and the weights of the perfect perceptron are:')
print(pp_weights)
```

## Step 3 and 4: produce sets for different P values and train the student perceptron

Now we generate the training sets. We will do this for a set of different $P$ values: 1, 10, 20, 50, 100, 150, 200. The goal is to understand whether we get an improvement in the learning process whenever we have a bigger training set size.

For each value of $P$, we do the following:

- set up a random vector of weights
- generate a random 0/1 matrix of size $P\times N$
- get the true values $\sigma_T^{\mu}$ for each $\mu$-th vector by passing the $\xi^{\mu}$ vector to the perfect perceptron (now this will be the teacher perceptron)
- iterate over variable $t$, that we max-bounded at 10000 in order to avoid cycles that are too long, looking for the situation in which the training error $\epsilon_t = 0$
- At each time $t$, iterate over the $\mu$-th data point and check whether the given weights (initalised at random) gives us the same results wrt those given by the teacher perceptron. If not, we *correct* the weights vector with a term given by the learning rule above.

What we would expect is that with bigger training sets the generalisation works better, and students are better able to reproduce the behaviour of the teacher since they are *exposed* more often to what is right and what is wrong.

```{python}
N = 20
prefact = 1./np.sqrt(N)

pp_weights = get_pp_weights(N)

final_weights = []

for P in [1, 10, 20, 50, 100, 150, 200]:
    random_weights = np.random.normal(0, 1, size=N)
    
    x = np.random.randint(0, 2, size=(P, N))
    y_teacher = perceptron(x, pp_weights)
    max_iter = 10_000
    
    for t in range(max_iter):
        epsilon_t = 0
        # for every t apply the learning rule
        # once weights are updated along mu, update the result
        y_student = perceptron(x, random_weights)
        # compute the current y_student value
        for mu in range(P):
            if (y_student[mu] != y_teacher[mu]):
                # update weights
                random_weights += prefact * y_teacher[mu] * x[mu] * (1 + np.random.normal(0, np.sqrt(50), size=N))
                epsilon_t += 1
        if (epsilon_t == 0):
            print('Training for P =', P, 'converged in time', t)
            break

    final_weights.append(random_weights)
```

```{python}
print('In ascending order of P weight vectors are')
list(map(lambda x: x.round(decimals=2), final_weights))
```

## Step 5: Evaluate single instance test error for 1000 elements

Now that we have trained the student perceptrons, we want to test them against out-of-sample data. We show now the results for a single instance of the $P_{test}=1000$ case.

```{python}
P_test = 1000

print('On a single instance:')

x_test = np.random.randint(0, 2, size=(P_test, N))
y_test_true = perceptron(x_test, pp_weights)

for P_idx, P in enumerate([1, 10, 20, 50, 100, 150, 200]):
    y_test_pred = perceptron(x_test, final_weights[P_idx])
    epsilon = 0
    for mu in range(P_test):
        if (y_test_pred[mu] != y_test_true[mu]):
            epsilon += 1
    
    print('- the test error for P =', P, 'is', epsilon*1./P_test)
```

We can clearly see that there is a particular behaviour as $P$ increases, in particular we see that the training error is decreasing. What happens if we do this many times? How will the training error be distributed over, say, a 1000 instantiations of the train-test process?

## Step 6: Do this 1000 times and evaluate the average test error

We can plot histograms of the result and see the distributions of test errors over a set of different $P$ values. This result doesn't come as totally unexpected: the bigger the training set, the lesser the errors we make. We could also see a hint of this by seeing that array elements of higher $P$ weight vectors were more and more similar to those of the perfect perceptron.

```{python}
def train_test_loop(i, P, N, max_iter=10_000):
    prefact = 1./np.sqrt(N)
    pp_weights = get_pp_weights(N)
    final_weights = []
    
    random_weights = np.random.normal(0, 1, size=(N))    
    x = np.random.randint(0, 2, size=(P, N))
    y_teacher = perceptron(x, pp_weights)
    
    # search for convergence (train_error=0)
    for t in range(max_iter):
        epsilon_t = 0
        # for every t apply the learning rule
        # once weights are updated along mu, update the result
        y_student = perceptron(x, random_weights)
        # compute the current y_student value
        for mu in range(P):
            if (y_student[mu] != y_teacher[mu]):
                # update weights
                random_weights += prefact * y_teacher[mu] * x[mu] * (1 + np.random.normal(0, np.sqrt(50), size=N))
                epsilon_t += 1
        if (epsilon_t == 0):
            #print('Training for P =', P, 'converged in time', t)
            break
    
    # now test once for the test_error over all the samples
    x_test = np.random.randint(0, 2, size=(P_test, N))
    y_test_true = perceptron(x_test, pp_weights)
    y_test_pred = perceptron(x_test, random_weights)
    epsilon_test = 0
    for mu in range(P_test):
        if (y_test_pred[mu] != y_test_true[mu]):
            epsilon_test += 1
    epsilon_test /= 1000.

    return epsilon_test
```

```{python}
# I define partial functions, where I have just to set the i and P arguments
ttloop20 = partial(train_test_loop, N=20)
ttloop40 = partial(train_test_loop, N=40)
```

```{python}
# number of cores available
n_cores = multiprocessing.cpu_count()
```

```{python}
Ps_N20 = np.array([1, 10, 20, 50, 100, 150, 200])
epsilon_means_N20 = []
epsilon_stds_N20 = []
for P in Ps_N20:
    start = time.time()
    epsilons = Parallel(n_jobs=n_cores)(delayed(ttloop20)(i, P=P) for i in range(1000))
    epsilon_mean = np.mean(epsilons)
    epsilon_std = np.std(epsilons) / np.sqrt(1000)
    epsilon_means_N20.append(epsilon_mean)
    epsilon_stds_N20.append(epsilon_std)
    end = time.time()
    print(f'For P={P}, eps=({round(epsilon_mean, 2)} +- {round(epsilon_std, 2)}), took {round(end-start)}s')
```

```{python}
# THIS WAS THE PREVIOUS CODE (1 TRAIN 1000 TEST)

# P_test = 1000

# plt.figure()
# plt.title('Histogram of test error for $p=1000$ done 1000 times')

# epsilons_P_means = []
# epsilons_P_stds = []

# for P_idx, P in enumerate([1, 10, 20, 50, 100, 150, 200]):
#     epsilons = []
#     for i in range(1000):
#         x_test = np.random.randint(0, 2, size=(P_test, N))
#         y_test_true = perceptron(x_test, pp_weights)
#         y_test_pred = perceptron(x_test, final_weights[P_idx])
#         epsilon = 0
#         for mu in range(P_test):
#             if (y_test_pred[mu] != y_test_true[mu]):
#                 epsilon += 1
#         epsilon /= 1000.
#         epsilons.append(epsilon)
#     plt.hist(epsilons, label=f'P = {P}', bins=10)
#     epsilons_P_means.append(np.mean(epsilons))
#     epsilons_P_stds.append(np.std(epsilons))

# plt.xlim([0, 1])
# plt.legend()
# plt.show()
# plt.close()
```

## Step 7: Plot $\epsilon$ vs. $P$

Now let us clearly state this by plotting the behaviour of the averages of the test error with respect to the progressively increasing size of the training set. As previously stated, it is unexpectedly decreasing.

```{python}
plt.figure()
plt.title('Test error vs. training set size')
plt.errorbar(Ps_N20, epsilon_means_N20, yerr=epsilon_stds_N20, fmt='.b', markersize=1)
plt.xlabel('$P$')
plt.ylabel('$\epsilon_{test}$')
plt.grid()
plt.show()
```

## Step 8: Repeat Steps 3-6 again but with $N=40$

Also notice that this time $P$ values are 1, 2, 20, 40, 100, 200, 300.

```{python}
N = 40

pp_weights = get_pp_weights(N)

print('Currently N is', N, 'and the weights of the perfect perceptron are:')
print(pp_weights)
```

```{python}
prefact = 1./np.sqrt(N)

final_weights = []

for P in [1, 2, 20, 40, 100, 200, 300]:
    random_weights = np.random.normal(0, 1, size=N)
    
    x = np.random.randint(0, 2, size=(P, N))
    y_teacher = perceptron(x, pp_weights)
    max_iter = 10_000
    
    for t in range(max_iter):
        epsilon_t = 0
        # for every t apply the learning rule
        # once weights are updated along mu, update the result
        y_student = perceptron(x, random_weights)
        # compute the current y_student value
        for mu in range(P):
            if (y_student[mu] != y_teacher[mu]):
                # update weights
                random_weights += prefact * y_teacher[mu] * x[mu] * (1 + np.random.normal(0, np.sqrt(50), size=N))
                epsilon_t += 1
        if (epsilon_t == 0):
            print('Training for P =', P, 'converged in time', t)
            break

    final_weights.append(random_weights)
```

```{python}
print('In ascending order of P weight vectors are')
list(map(lambda x: x.round(decimals=2), final_weights))
```

```{python}
P_test = 1000

print('On a single instance:')

x_test = np.random.randint(0, 2, size=(P_test, N))
y_test_true = perceptron(x_test, pp_weights)

for P_idx, P in enumerate([1, 2, 20, 40, 100, 200, 300]):
    y_test_pred = perceptron(x_test, final_weights[P_idx])
    epsilon = 0
    for mu in range(P_test):
        if (y_test_pred[mu] != y_test_true[mu]):
            epsilon += 1
    
    print('- the test error for P =', P, 'is', epsilon*1./P_test)
```

```{python}
Ps_N40 = np.array([1, 2, 20, 40, 100, 200, 300])
epsilon_means_N40 = []
epsilon_stds_N40 = []
for P in Ps_N40:
    start = time.time()
    epsilons = Parallel(n_jobs=n_cores)(delayed(ttloop40)(i, P=P) for i in range(1000))
    epsilon_mean = np.mean(epsilons)
    epsilon_std = np.std(epsilons) / np.sqrt(1000)
    epsilon_means_N40.append(epsilon_mean)
    epsilon_stds_N40.append(epsilon_std)
    end = time.time()
    print(f'For P={P}, eps=({round(epsilon_mean, 2)} +- {round(epsilon_std, 2)}), took {round(end-start)}s')
```

```{python}
# THIS WAS THE PREVIOUS CODE (1 TRAIN 1000 TEST)

# P_test = 1000

# plt.figure()
# plt.title('Histogram of test error for $p=1000$ done 1000 times')

# epsilons_P_N40_means = []
# epsilons_P_N40_stds = []

# for P_idx, P in enumerate([1, 2, 20, 40, 100, 200, 300]):
#     epsilons = []
#     for i in range(1000):
#         x_test = np.random.randint(0, 2, size=(P_test, N))
#         y_test_true = perceptron(x_test, pp_weights)
#         y_test_pred = perceptron(x_test, final_weights[P_idx])
#         epsilon = 0
#         for mu in range(P_test):
#             if (y_test_pred[mu] != y_test_true[mu]):
#                 epsilon += 1
#         epsilon /= 1000.
#         epsilons.append(epsilon)
#     plt.hist(epsilons, label=f'P = {P}', bins=10)
#     epsilons_P_N40_means.append(np.mean(epsilons))
#     epsilons_P_N40_stds.append(np.std(epsilons))

# plt.xlim([0, 1])
# plt.legend()
# plt.show()
# plt.close()
```

## Step 9: Plot $\epsilon$ vs. $P$ for $N=40, 20$

Let us compare the two plots.

```{python}
plt.figure()
plt.title('Test error vs. training set size for $N=20,40$')
plt.errorbar(Ps_N20, epsilon_means_N20, yerr=epsilon_stds_N20, fmt='.b', label='$N=20$')
plt.errorbar(Ps_N40, epsilon_means_N40, yerr=epsilon_stds_N40, fmt='.r', label='$N=40$')
plt.xlabel('$P$')
plt.ylabel('$\epsilon_{test}$')
plt.legend()
plt.grid()
plt.show()
```

## Step 10: Find a way to represent the data

In order to superimpose the two plots we may play with a rescaling of the x-axis, for example:

```{python}
plt.figure()
plt.title('Test error vs. training set size for $N=20,40$')
plt.errorbar(2*Ps_N20, epsilon_means_N20, yerr=epsilon_stds_N20, fmt='.b', markersize=1, label='$N=20$ with $P$\'s multiplied by a factor 2')
plt.errorbar(Ps_N40, epsilon_means_N40, yerr=epsilon_stds_N40, fmt='.r', markersize=1, label='$N=40$')
plt.xlabel('$P$')
plt.ylabel('$\epsilon_{test}$')
plt.legend()
plt.grid()
plt.show()
```

In this case we used a factor 2. This means that for double the input vector size, the $P$ values required to reach the same amount of test error convergence are the double. This is what we expect from the fact that (see, for example, Engel & Van den Broek p. 12):

$$
P = \alpha N
$$

```{python}
# p20 = np.polyfit([1, 10, 20, 50, 100, 150, 200], np.log(epsilons_P), 1)
# p40 = np.polyfit([1, 2, 20, 40, 100, 200, 300], np.log(epsilons_P_N40), 1)
# f20 = np.poly1d(p20)
# f40 = np.poly1d(p40)

# plt.figure()
# plt.title('Test error vs. training set size for $N=20,40$')
# plt.scatter([1, 10, 20, 50, 100, 150, 200], np.log(epsilons_P), label='$N=20$')
# plt.scatter([1, 2, 20, 40, 100, 200, 300], np.log(epsilons_P_N40), label='$N=40$')
# plt.plot([1, 10, 20, 50, 100, 150, 200], f20([1, 10, 20, 50, 100, 150, 200]))
# plt.plot([1, 2, 20, 40, 100, 200, 300], f40([1, 2, 20, 40, 100, 200, 300]))
# plt.xlabel('$P$')
# plt.ylabel('$\log\epsilon_{test}$')
# plt.legend()
# plt.grid()
# plt.show()

# print('The slopes are', p20[0], p40[0])
# print('The inverse of the slopes (decay time in terms of P) are', -1./p20[0], -1./p40[0])
```

Yet another way (suggested by the teacher) to rescale is to divide the $x$ axis by $N$, in order to have values that do not depend on $N$ anymore. This yields the same behaviour seen in the first example.

```{python}
plt.figure()
plt.title('Test error vs. training set size for $N=20,40$')
plt.errorbar(Ps_N20/20, epsilon_means_N20, yerr=epsilon_stds_N20, fmt='.b', label='$N=20$', markersize=1)
plt.errorbar(Ps_N40/40, epsilon_means_N40, yerr=epsilon_stds_N40, fmt='.r', label='$N=40$', markersize=1)
plt.xlabel('α')
plt.ylabel('$\epsilon_{test}$')
plt.legend()
plt.grid()
plt.show()
```

```{python}
np.savez('ex03_n20.npz', Ps_N20, epsilon_means_N20, epsilon_stds_N20)
np.savez('ex03_n40.npz', Ps_N40, epsilon_means_N40, epsilon_stds_N40)
```

