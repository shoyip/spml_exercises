---
title: 'Exercise 06: The Hebb''s rule and the perceptron rule'
jupyter: python3
---

```{python}
import numpy as np
from scipy.special import erfc
import scipy.integrate as integrate
import matplotlib.pyplot as plt
from functools import partial
import time
from joblib import Parallel, delayed
```

```{python}
alpha_ann, epsilon_ann = list(np.load('/home/shoichi/Documenti/FisicaLM/Cammarota/spml_exercises/notebooks/ex04.npz').values())
alpha_que, epsilon_que = list(np.load('/home/shoichi/Documenti/FisicaLM/Cammarota/spml_exercises/notebooks/ex05.npz').values())
```

```{python}
Ps_N40, epsilon_means_N40, epsilon_stds_N40 = list(np.load('/home/shoichi/Documenti/FisicaLM/Cammarota/spml_exercises/notebooks/ex03_n40.npz').values())
```


Let us take, as always, into account the classification problem for $N=40$.

The perceptron is given by
$$
\sigma(\vec \xi^{\mu}, \vec J) = \text{sign}(\vec \xi^{\mu} \cdot \vec J).
$$

In the **training**, we initialise the student perceptrons at random. They will attempt learning with a **learning rule**.

In Exercise 03 we used the following rule:
$$
\vec J (t+1) = \vec J + \frac{1}{\sqrt{N}} \sigma^{\mu}_T \vec \xi^{\mu} (1+ r)
$$
which is called the **randomised perceptron rule**. This time, we are asked to do the same but with another learning rule, namely the **Hebb's rule**, given by
$$
\vec J^H = \frac{1}{\sqrt{N}} \sum_{\mu=1}^P \vec \xi^{\mu} \sigma_T^{\mu}
$$
which is not anymore an *iterative rule*. So this operation alone will account for the training, given a training set, and we will not have to look for a convergence.

Since we previously did the computation for a teacher which was taken to be the perfect perceptron, we might start with the same use case. But keep in mind that the choice of the teacher is arbitrary and does not influence the performance of the learning.


```{python}
def my_sign(x):
    # x is a numpy array
    return 2 * (x >= 0) - 1

def get_pp_weights(N):
    if N%2!=0:
        raise ValueError('N value should be even and not odd.')
    half_N = int(N/2)
    J_star = np.zeros(N)
    for i in range(1, N+1):
        J_star[i-1] = 2**(half_N - i) if i<=half_N else -J_star[i-(half_N+1)]
    return J_star

def perceptron(x, weights):
    return my_sign(x @ weights)
```

```{python}
def J_Hebb(x_train, y_train):
    # x_train is a matrix where rows are mu-count (0, P) and columns are i-count (0, N)
    # y_train is a vector with mu-count (0, P)
    # returns a J weights vector with i-count (0, N)
    N = x_train.shape[1]
    return np.sum(x_train * y_train.reshape(-1,1), axis=0) / np.sqrt(N)
```

```{python}
pp_weights = get_pp_weights(40)
```

```{python}
N=40
P=10
x_train = np.random.randint(0, 2, size=(P, N))
y_train = perceptron(x_train, pp_weights)
# THIS SHOULD HOLD FOR THE MORE GENERAL CASE
# IN WHICH WE DO NOT HAVE THE PERFECT PERCEPTRON
# BUT A RANDOM (QUENCHED) TEACHER
# y_train = np.random.randint(0, 2, size=(P))
```

```{python}
eps_test_means_list = []
eps_test_stds_list = []
eps_train_means_list = []
eps_train_stds_list = []
alphas_list = []
P_test = 1000

for P in np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 40, 100, 200, 300, 400, 600, 800, 1000, 2000, 3500, 4500]):
    err_test_list = []
    err_train_list = []
    for tt_loop in range(1000):
        # initialize random (P, N) training set data
        x_train = np.random.randint(0, 2, size=(P, N)) * 2 - 1
        # initalize random (P) training set labels
        y_train_true = perceptron(x_train, pp_weights)
        #y_train_true = np.random.randint(0, 2, size=(P)) * 2 - 1
        # perform Hebb's learning
        J_H = J_Hebb(x_train, y_train_true) # weights
        y_train_pred = perceptron(x_train, J_H)
        err_train = np.sum(np.abs(y_train_pred-y_train_true))/(2*P)
        #print('training error:', err_train)
        err_train_list.append(err_train)
    
        x_test = np.random.randint(0, 2, size=(P_test, N)) * 2 - 1
        y_test_pred = perceptron(x_test, J_H)
        y_test_true = perceptron(x_test, pp_weights)
        err_test = np.sum(np.abs(y_test_pred-y_test_true))/(2*P_test)
        #print('test error:', err_test)
        err_test_list.append(err_test)
    
    errs_train = np.array(err_train_list)
    errs_test = np.array(err_test_list)
    eps_train_means_list.append(np.mean(errs_train))
    eps_train_stds_list.append(np.std(errs_train)/np.sqrt(P))
    eps_test_means_list.append(np.mean(errs_test))
    eps_test_stds_list.append(np.std(errs_test)/np.sqrt(P_test))
    alphas_list.append(P / N)
    #print(np.mean(errs_test))
```

```{python}
eps_train_means, eps_train_stds, eps_test_means, eps_test_stds, alphas = map(np.array, [eps_train_means_list, eps_train_stds_list, eps_test_means_list, eps_test_stds_list, alphas_list])
```

```{python}
def eps(alpha):
    return np.arccos(np.sqrt(2*alpha/(2*alpha + np.pi))) / np.pi

def eps_train(alpha, lim=np.infty):
    integrate_result = integrate.quad(lambda x: H_func(1 / np.sqrt(alpha) + np.sqrt(alpha) * x), 0, +lim)
    if integrate_result[1] < 0.001:
        return integrate_result[0]
    else:
        raise ValueError(f'Error of numerical integral is higher than 0.001: {round(integrate_result[1], 4)}')

def H_func(u):
    return erfc(u/np.sqrt(2)) / 2.

veps_train = np.vectorize(eps_train)
```

```{python}
linalpha = np.linspace(0.01, 130, 1000)
eps_linalpha = eps(linalpha)
eps_train_linalpha = veps_train(linalpha)
```

## Step 1, 2 and 3: Apply Hebb's rule, perform train test loop and compare with analytical results

We do the same we did for the randomised perceptron rule with the Hebb's rule: we perform 1000 loops of training and tests, with varying values of training set size $P$ and with $P_{\text{test}}=1000$.

Then, we superimpose the numerical results from the computation of the analytical functions
$$
\epsilon(\alpha) = \frac{1}{\pi} \arccos(\sqrt{\frac{2\alpha}{2\alpha+\pi}}) \qquad \epsilon_{\text{train}}(\alpha) = 2 \int_0^{+\infty} dx \; H(\frac{1}{\sqrt{\alpha}} + \sqrt{\frac{2\alpha}{\pi}} x)
$$
with
$$
H(u) \int_u^{+\infty} \frac{dy}{2\pi} \; \exp(-y^2/2).
$$

We get the following plot:

```{python}
plt.figure()
plt.errorbar(alphas, eps_test_means, yerr=eps_test_stds, fmt='.', label='Hebb Test')
plt.errorbar(alphas, eps_train_means, yerr=eps_train_stds, fmt='.', label='Hebb Training')
plt.plot(linalpha, eps_linalpha, label='Hebb Analytical Test')
plt.plot(linalpha, eps_train_linalpha, label='Hebb Analytical Training')
plt.legend()
plt.grid()
plt.show()
```

## Step 4: Small and large behaviour of $\epsilon$, $\epsilon_{\text{train}}$ and their difference

The behaviour of $\epsilon$ and $\epsilon_{\text{train}}$ are given by the plot above. We can observe that:

- the training error is not null (this doesn't happen, for instance, in the case of the perceptron rule since we aim for convergence to zero training error)
- the downward slope for small values of $\alpha$ is much steeper with respect to that of the perceptron learning.
- since a finite training error is present, for large $P$ values the generalization error converges towards the curve of the training error and hence seems to decrease less rapidly with respect to the case of the perceptron learning.

From the analytic expectation, we can try to fit it with the $\alpha$-small or large limit expansion and see whether we get the correct prefactors. For small alpha, we get that
$$
\frac{1}{2} - \epsilon \sim \frac{\sqrt{2}}{\pi^{3/2}} \; \sqrt{\alpha}
$$

```{python}
xs = np.linspace(0, 2.5, 200)
ys = 1./2. - np.sqrt(2*xs)/np.pi**(3./2.)
plt.figure()
plt.errorbar(alphas, eps_test_means, yerr=eps_test_stds, fmt='.', label='Hebb Test')
plt.plot(xs, ys, label=r'Small $\alpha$ expansion')
plt.xlabel('Alpha')
plt.ylabel('Generalization Error')
plt.legend()
plt.grid()
plt.show()
```

We can then compute the slope between $\frac{1}{2} - \epsilon$ vs. $\sqrt{\alpha}$ by fitting the function. We get the following result:

```{python}
from scipy.optimize import curve_fit
```

```{python}
x_err_test = np.sqrt(alphas)
y_err_test = .5 - eps_test_means

x_fit = x_err_test[:10]
y_fit = y_err_test[:10]

popt, pcov = curve_fit(lambda x, m, q: m*x+q, x_fit, y_fit)
```

```{python}
print(f"Slope of the fit given by the first 10 points is {popt[0]}")
```

which is compatible with the expected result:
$$
\frac{\sqrt{2}}{\pi^{3/2}} \sim 0.25397.
$$

The difference $\epsilon - \epsilon_{\text{train}}$ at large $\alpha$ values behaves like the following:

```{python}
plt.figure()
plt.errorbar(alphas, (eps_test_means-eps_train_means), fmt='.', label='Hebb $\epsilon-\epsilon_{{train}}$')
plt.legend()
plt.grid()
plt.show()
```

We see that at large $\alpha$ values, the difference between the generalization and training error gets flatter and flatter. We expect that at large values they will behave in the same manner. We can check this by looking at finite differences between successive datapoints:

```{python}
plt.figure()
plt.errorbar(alphas[1:], (eps_test_means-eps_train_means)[1:] - (eps_test_means-eps_train_means)[:-1], fmt='.', label='Hebb $\Delta \epsilon[i] - \Delta \epsilon[i-1]$ ')
plt.legend()
plt.grid()
plt.show()
```

The finite successive differences have a good behaviour, since they converge to value approximately zero. This means that we expect our derivative to get to zero, meaning that the difference between the two errors becomes constant. Since
$$
\epsilon_t \sim \epsilon \sim \frac{1}{\sqrt{2\pi\alpha}}
$$
is the behaviour we expect, the two results are compatible for, say, $\alpha > 20$.

## Step 5: compare Hebbs, randomised perceptron and Gibbs result

We understood in Exercise 05 that Gibbs learning is best represented by the quenched computation. Then let us compare those results with the simulations of the randomised perceptron rule (Exercise 03) and Hebb's rule.

```{python}
plt.figure()
plt.errorbar(alpha_que, epsilon_que, fmt='-', label='Gibbs learning quenched error')
plt.errorbar(alphas, eps_test_means, fmt='x', label='Hebb\'s rule generalization error')
plt.errorbar(Ps_N40/40, epsilon_means_N40, fmt='o', label='Randomised perceptron error')
plt.xlim([-1, 10])
plt.grid()
plt.legend()
plt.xlabel(r'$\alpha$')
plt.ylabel(r'$\epsilon$')
plt.show()

#alpha_que, epsilon_que
```

We can notice that at low values of $\alpha$ the generalization error in Hebb's rule decreases much faster with respect to the behaviour of the error in the case of Gibbs rule. The results from the randomised perceptron error lie, as we previously saw, on the error curve of the quenched computations for the Gibbs learning.

## Step 6, 7 and 8: apply the (nonrandomised) perceptron rule and discuss the results

We will now perform the same type of analysis that we performed for the Hebbs rule and Gibbs rule (both in the annealed and quenched scenarios) by iterating 1000 times the train-test cycle over multiple values of $P$ and evaluating the resulting behaviour of the generalization error.

```{python}
def train_test_loop(i, P, N, max_iter=10_000):
    prefact = 1./np.sqrt(N)
    pp_weights = get_pp_weights(N)
    final_weights = []
    
    random_weights = np.random.normal(0, 1, size=(N))    
    x = np.random.randint(0, 2, size=(P, N))
    y_teacher = perceptron(x, pp_weights)
    
    # search for convergence (train_error=0)
    for t in range(max_iter):
        epsilon_t = 0
        # for every t apply the learning rule
        # once weights are updated along mu, update the result
        y_student = perceptron(x, random_weights)
        # compute the current y_student value
        for mu in range(P):
            if (y_student[mu] != y_teacher[mu]):
                # update weights
                random_weights += prefact * y_teacher[mu] * x[mu] #* (1 + np.random.normal(0, np.sqrt(50), size=N))
                epsilon_t += 1
        if (epsilon_t == 0):
            #print('Training for P =', P, 'converged in time', t)
            break
    
    # now test once for the test_error over all the samples
    x_test = np.random.randint(0, 2, size=(P_test, N))
    y_test_true = perceptron(x_test, pp_weights)
    y_test_pred = perceptron(x_test, random_weights)
    epsilon_test = 0
    for mu in range(P_test):
        if (y_test_pred[mu] != y_test_true[mu]):
            epsilon_test += 1
    epsilon_test /= 1000.

    return epsilon_test
```

```{python}
ttloop40 = partial(train_test_loop, N=40)
```

```{python}
Ps_N40_nr = np.array([2, 20, 40, 100, 200, 300])
epsilon_means_N40_nr = []
epsilon_stds_N40_nr = []
for P in Ps_N40_nr:
    start = time.time()
    epsilons = Parallel(n_jobs=4)(delayed(ttloop40)(i, P=P) for i in range(1000))
    epsilon_mean = np.mean(epsilons)
    epsilon_std = np.std(epsilons) / np.sqrt(1000)
    epsilon_means_N40_nr.append(epsilon_mean)
    epsilon_stds_N40_nr.append(epsilon_std)
    end = time.time()
    print(f'For P={P}, eps=({round(epsilon_mean, 2)} +- {round(epsilon_std, 2)}), took {round(end-start)}s')
```

```{python}
plt.figure()
plt.errorbar(Ps_N40_nr / 40, epsilon_means_N40_nr, yerr=epsilon_stds_N40_nr, fmt='.-', label='Nonrandomised Perceptron Rule')
plt.errorbar(Ps_N40 / 40, epsilon_means_N40, yerr=epsilon_stds_N40, fmt='.-', label='Randomised Perceptron Rule')
plt.errorbar(alphas, eps_test_means, yerr=eps_train_stds, fmt='.-', label='Hebb Rule')
plt.legend()
plt.grid()
plt.xlim([-.5, 9])
plt.xlabel(r'$\alpha$')
plt.ylabel(r'$\epsilon$')
plt.show()
```

We can clearly see that:

- as pointed out in Step 5, the Hebb's rule performs well at low $\alpha$'s and worse at high $\alpha$'s with respect to both of the other perceptron rule results
- the **nonrandomised** perceptron rule generalization error is _lower_ than the randomised one over higher and higher values of $\alpha$.

We can hence conclude that the nonrandomised perceptron rule yields to better results in terms of generalization error because the noise term does not slow down the learning process, leading to a faster convergence to the zero training error over the dataset.

However, without the noise term the comparison with results from statistical physics is not possible anymore, because computations with the saddle point method are possible thanks to the presence of noise that allows a better sampling over the configuration space.

